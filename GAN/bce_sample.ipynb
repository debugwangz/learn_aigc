{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-14T04:46:17.693487Z",
     "start_time": "2025-06-14T04:46:16.879574Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.io import loadmat\n",
    "from torch import nn\n",
    "from gan import Generator, Discriminator"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T04:46:17.742412Z",
     "start_time": "2025-06-14T04:46:17.697617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ],
   "id": "9b7d32d2b6e5fe4a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T04:46:18.741632Z",
     "start_time": "2025-06-14T04:46:18.739020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data, label, transform=None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.data[idx]), self.label[idx]"
   ],
   "id": "1ed697aa6b5162f2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T04:46:19.802311Z",
     "start_time": "2025-06-14T04:46:19.624345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist = loadmat(\"../mnist-original.mat/mnist-original.mat\")\n",
    "mnist_data = mnist[\"data\"].T\n",
    "mnist_label = mnist[\"label\"][0]"
   ],
   "id": "23b29e071c176ac0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T04:46:26.380524Z",
     "start_time": "2025-06-14T04:46:26.378343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 10\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "lr = 2e-4\n",
    "k = 1\n",
    "latent_dim = 128\n"
   ],
   "id": "14fbff80a4280cde",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T04:46:27.342114Z",
     "start_time": "2025-06-14T04:46:27.141069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator = Generator(latent_dim).to(device)\n",
    "generator.load_state_dict(torch.load( f\"/mnt/d/data/mnist_model/GAN_BCE/g_lr_{lr}_epoch_{EPOCHS}_latent{latent_dim}_k_{k}.pth\", map_location=device))"
   ],
   "id": "d48fe3a19499d022",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25016/3169254674.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load( f\"/mnt/d/data/mnist_model/GAN_BCE/g_lr_{lr}_epoch_{EPOCHS}_latent{latent_dim}_k_{k}.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T04:46:29.727662Z",
     "start_time": "2025-06-14T04:46:29.577134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = torch.empty(10000, 1, 28, 28)\n",
    "result = result.to(device)\n",
    "with torch.no_grad():\n",
    "    generator.eval()\n",
    "    # Generate 100 batches of 100 images each\n",
    "    for i in range(100):\n",
    "        print(f\"Generating batch {i+1}/100\")\n",
    "        z = torch.randn(100, generator.latent_dim).to(device)\n",
    "        output = generator(z)\n",
    "        result[i*100:(i+1)*100] = output\n",
    "    "
   ],
   "id": "c04b22ef86ed834e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating batch 1/100\n",
      "Generating batch 2/100\n",
      "Generating batch 3/100\n",
      "Generating batch 4/100\n",
      "Generating batch 5/100\n",
      "Generating batch 6/100\n",
      "Generating batch 7/100\n",
      "Generating batch 8/100\n",
      "Generating batch 9/100\n",
      "Generating batch 10/100\n",
      "Generating batch 11/100\n",
      "Generating batch 12/100\n",
      "Generating batch 13/100\n",
      "Generating batch 14/100\n",
      "Generating batch 15/100\n",
      "Generating batch 16/100\n",
      "Generating batch 17/100\n",
      "Generating batch 18/100\n",
      "Generating batch 19/100\n",
      "Generating batch 20/100\n",
      "Generating batch 21/100\n",
      "Generating batch 22/100\n",
      "Generating batch 23/100\n",
      "Generating batch 24/100\n",
      "Generating batch 25/100\n",
      "Generating batch 26/100\n",
      "Generating batch 27/100\n",
      "Generating batch 28/100\n",
      "Generating batch 29/100\n",
      "Generating batch 30/100\n",
      "Generating batch 31/100\n",
      "Generating batch 32/100\n",
      "Generating batch 33/100\n",
      "Generating batch 34/100\n",
      "Generating batch 35/100\n",
      "Generating batch 36/100\n",
      "Generating batch 37/100\n",
      "Generating batch 38/100\n",
      "Generating batch 39/100\n",
      "Generating batch 40/100\n",
      "Generating batch 41/100\n",
      "Generating batch 42/100\n",
      "Generating batch 43/100\n",
      "Generating batch 44/100\n",
      "Generating batch 45/100\n",
      "Generating batch 46/100\n",
      "Generating batch 47/100\n",
      "Generating batch 48/100\n",
      "Generating batch 49/100\n",
      "Generating batch 50/100\n",
      "Generating batch 51/100\n",
      "Generating batch 52/100\n",
      "Generating batch 53/100\n",
      "Generating batch 54/100\n",
      "Generating batch 55/100\n",
      "Generating batch 56/100\n",
      "Generating batch 57/100\n",
      "Generating batch 58/100\n",
      "Generating batch 59/100\n",
      "Generating batch 60/100\n",
      "Generating batch 61/100\n",
      "Generating batch 62/100\n",
      "Generating batch 63/100\n",
      "Generating batch 64/100\n",
      "Generating batch 65/100\n",
      "Generating batch 66/100\n",
      "Generating batch 67/100\n",
      "Generating batch 68/100\n",
      "Generating batch 69/100\n",
      "Generating batch 70/100\n",
      "Generating batch 71/100\n",
      "Generating batch 72/100\n",
      "Generating batch 73/100\n",
      "Generating batch 74/100\n",
      "Generating batch 75/100\n",
      "Generating batch 76/100\n",
      "Generating batch 77/100\n",
      "Generating batch 78/100\n",
      "Generating batch 79/100\n",
      "Generating batch 80/100\n",
      "Generating batch 81/100\n",
      "Generating batch 82/100\n",
      "Generating batch 83/100\n",
      "Generating batch 84/100\n",
      "Generating batch 85/100\n",
      "Generating batch 86/100\n",
      "Generating batch 87/100\n",
      "Generating batch 88/100\n",
      "Generating batch 89/100\n",
      "Generating batch 90/100\n",
      "Generating batch 91/100\n",
      "Generating batch 92/100\n",
      "Generating batch 93/100\n",
      "Generating batch 94/100\n",
      "Generating batch 95/100\n",
      "Generating batch 96/100\n",
      "Generating batch 97/100\n",
      "Generating batch 98/100\n",
      "Generating batch 99/100\n",
      "Generating batch 100/100\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T04:46:31.765081Z",
     "start_time": "2025-06-14T04:46:31.744464Z"
    }
   },
   "cell_type": "code",
   "source": "result = (result-result.min())/(result.max()-result.min())",
   "id": "cf0c537a73b1cd11",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T04:52:31.320201Z",
     "start_time": "2025-06-14T04:51:35.191043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tifffile as tif\n",
    "import os\n",
    "result_dir = \"/mnt/d/data/mnist_result/gan_bce_result/\"\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "from os.path import join as ospj\n",
    "for i in range(result.shape[0]):\n",
    "    tif.imwrite(ospj(result_dir, f\"{str(i).zfill(5)}.tif\"), result[i].squeeze().cpu().numpy())"
   ],
   "id": "f33da6adaca8ec00",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T04:54:14.328474Z",
     "start_time": "2025-06-14T04:52:31.372336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dir = \"/mnt/d/data/mnist_result/test_sample/\"\n",
    "result_dir = \"/mnt/d/data/mnist_result/gan_bce_result/\"\n",
    "\n",
    "real_images_folder = test_dir\n",
    "# generated_images_folder = './FID_app3'\n",
    "generated_images_folder = result_dir\n",
    "import torch\n",
    "from pytorch_fid import fid_score\n",
    "\n",
    "fid_value = fid_score.calculate_fid_given_paths([real_images_folder, generated_images_folder],\n",
    "                                                batch_size=50,\n",
    "                                                device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "                                                dims=2048                                                 \n",
    "                                                )\n",
    "print(\"FID score:\", fid_value)"
   ],
   "id": "6ebf941cd04b317",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:44<00:00,  4.50it/s]\n",
      "100%|██████████| 200/200 [00:54<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID score: 2.8555386485814287\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5753778a4dab8c00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
